# Ход выполнения ДЗ к занятию "10.06. Инцидент-менеджмент"

## Задание 1

Составьте постмотрем, на основе реального сбоя системы Github в 2018 году.

Информация о сбое доступна [в виде краткой выжимки на русском языке](https://habr.com/ru/post/427301/) , а
также [развёрнуто на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).

## Перевод развернутой информации о сбое

На прошлой неделе в GitHub произошел инцидент, который привел к снижению качества обслуживания на 24 часа 11 минут. Хотя часть нашей платформы не пострадала от этого инцидента, **были затронуты несколько внутренних систем, что привело к отображению устаревшей и непоследовательной информации**. В конечном итоге никакие пользовательские данные не были потеряны; однако **ручное согласование нескольких секунд записи в базу данных все еще выполняется**. В большинстве случаев **GitHub также не мог обслуживать события веб-перехватчиков или создавать и публиковать сайты GitHub Pages**.

Все мы в GitHub хотели бы искренне извиниться за влияние, которое это оказало на каждого из вас. Мы знаем о вашем доверии к GitHub и гордимся тем, что создаем устойчивые системы, которые позволяют нашей платформе оставаться высокодоступной. С этим инцидентом мы подвели вас, и мы глубоко сожалеем. Хотя **мы не можем устранить проблемы, вызванные тем, что платформа GitHub не использовалась в течение длительного периода времени**, мы можем объяснить события, которые привели к этому инциденту, уроки, которые мы извлекли, и шаги, которые мы предпринимаем как компания для лучше убедиться, что это не повторится.

#### Фон
Большинство пользовательских сервисов GitHub запускаются в наших собственных центрах обработки данных . 

* Предпосылки - критически важное упущение

Топология центра обработки данных предназначена для обеспечения надежной и расширяемой пограничной сети, которая работает перед несколькими региональными центрами обработки данных, обеспечивающими наши рабочие нагрузки вычислений и хранения. **Несмотря на уровни избыточности, встроенные в физические и логические компоненты в этой конструкции, все еще возможно, что сайты не смогут взаимодействовать друг с другом в течение некоторого времени**.

#### 21 октября в 22:52 UTC 
* Причина
**в результате регламентных работ по замене вышедшего из строя оптического оборудования 100G была потеряна связь между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США. Связь между этими точками была восстановлена за 43 секунды, но этот кратковременный сбой вызвал цепочку событий, которые привели к ухудшению качества обслуживания на 24 часа 11 минут**.

В прошлом мы обсуждали, как мы используем MySQL для хранения метаданных GitHub , а также наш подход к MySQL High Availability . **GitHub управляет несколькими кластерами MySQL размером от сотен гигабайт до почти пяти терабайт, каждый из которых имеет до десятков реплик чтения на кластер для хранения метаданных, отличных от Git, поэтому наши приложения могут предоставлять запросы на вытягивание и проблемы, управлять аутентификацией, координировать фоновую обработку, а также предоставлять дополнительные функции, помимо необработанного хранилища объектов Git. Различные данные в разных частях приложения хранятся в разных кластерах посредством функционального сегментирования.**

* Воздействие
Чтобы повысить производительность при масштабировании, наши приложения будут направлять записи на соответствующий первичный сервер для каждого кластера, но в подавляющем большинстве случаев делегируют запросы на чтение подмножеству серверов-реплик. **Мы используем Orchestrator для управления топологиями наших кластеров MySQL и автоматического аварийного переключения**. Orchestrator учитывает ряд переменных во время этого процесса и построен на основе Raft для достижения консенсуса. **Orchestrator может реализовывать топологии, которые приложения не могут поддерживать, поэтому необходимо позаботиться о согласовании конфигурации Orchestrator с ожиданиями на уровне приложений**.

### Хронология инцидента

#### 2018 21 октября 22:52 UTC
* Воздействие
Согласно консенсусу Raft, во время описанного выше раздела сети Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отмены выбора руководства. Центр обработки данных на западном побережье США и узлы Orchestrator общедоступного облака на восточном побережье США смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США. Orchestrator приступил к организации топологии кластера базы данных Западного побережья США. Когда подключение было восстановлено, наш уровень приложений немедленно начал направлять трафик записи на новые первичные серверы на сайте West Coast.

* Воздействие
Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который не был реплицирован на объект на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить возврат основного сервера в центр обработки данных на восточном побережье США.

#### 2018 21 октября 22:54 UTC
* Обнаружение
Наши внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. К 23:02 UTC инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. Запрос API Orchestrator показал топологию репликации базы данных, которая включала только серверы из нашего центра обработки данных на западном побережье США.

#### 2018 21 октября 23:07 UTC
* Реакция
К этому моменту отвечающая команда решила вручную заблокировать наш внутренний инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. В 23:09 по всемирному координированному времени команда респондентов поместила сайт в желтый статус . Это действие автоматически переводит ситуацию в активный инцидент и отправляет предупреждение координатору инцидентов. В 23:11 по всемирному координированному времени присоединился координатор инцидента и через две минуты изменил статус решения на красный .

#### 2018 21 октября 23:13 UTC
* Реакция
В то время было понятно, что проблема затронула несколько кластеров баз данных. Были вызваны дополнительные инженеры из группы разработки баз данных GitHub. Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. Это было непросто, потому что к этому моменту кластер базы данных West Coast уже почти 40 минут принимал записи с нашего уровня приложений. Кроме того, в кластере Восточного побережья существовало несколько секунд операций записи, которые не были реплицированы на Западное побережье, что предотвратило репликацию новых операций записи обратно на Восточное побережье.

* Пояснение причин
Защита конфиденциальности и целостности пользовательских данных является наивысшим приоритетом GitHub. Стремясь сохранить эти данные, мы решили, что более 30 минут данных, записанных в центр обработки данных на западном побережье США, не позволили нам рассмотреть другие варианты, кроме переадресации при сбое, для обеспечения безопасности пользовательских данных. 

* Пояснение причин
Однако приложения, работающие на восточном побережье и зависящие от записи информации в кластер MySQL на западном побережье, в настоящее время не в состоянии справиться с **дополнительной задержкой**, возникающей из-за обмена данными между странами для большинства их вызовов базы данных. Это решение приведет к тому, что наш сервис станет непригодным для многих пользователей. Мы считаем, что длительная деградация сервиса стоила обеспечения согласованности данных наших пользователей.

#### 2018 21 октября 23:19 UTC
* Реакция
Из запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично ухудшить удобство использования сайта, приостановив доставку веб-перехватчика и сборки GitHub Pages вместо того, чтобы подвергать опасности данные, которые мы уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы отдавать предпочтение целостности данных, а не удобству использования сайта и времени восстановления.

#### 22 октября 2018 г., 00:05 UTC
* Реакция
Инженеры, участвующие в группе реагирования на инциденты, **начали разработку плана по устранению несоответствий данных** и внедрению наших процедур аварийного переключения для MySQL. Наш план состоял в том, чтобы **восстановить из резервных копий**, **синхронизировать реплики на обоих сайтах**, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди. 
Мы **обновили наш статус** , чтобы проинформировать пользователей о том, что мы собираемся выполнить контролируемый переход на другой ресурс внутренней системы хранения данных.

* Пояснение
Хотя резервные копии данных MySQL создаются каждые четыре часа и хранятся в течение многих лет, резервные копии хранятся удаленно в общедоступном облачном хранилище BLOB-объектов. **Время, необходимое для восстановления нескольких терабайт данных резервной копии, привело к тому, что процесс занял несколько часов**. Значительная часть времени ушла на перенос данных из службы удаленного резервного копирования. **Процесс распаковки, контрольной суммы, подготовки и загрузки больших файлов резервных копий на недавно подготовленные серверы MySQL занял большую часть времени**. Эта процедура тестируется как минимум ежедневно, поэтому сроки восстановления были хорошо известны, однако до этого инцидента нам никогда не приходилось полностью восстанавливать весь кластер из резервной копии, и вместо этого мы могли полагаться на другие стратегии, такие как отложенные реплики.

#### 22 октября 2018 г., 00:41 UTC
* Реакция
К этому времени был инициирован процесс резервного копирования для всех затронутых кластеров MySQL, и инженеры следили за его ходом. Одновременно несколько групп инженеров искали способы ускорить передачу и время восстановления без дальнейшего ухудшения удобства использования сайта или риска повреждения данных.

#### 22 октября 2018 г., 06:51 UTC
* Восстановление
Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья. Это привело к медленной загрузке сайта для страниц, которые должны были выполнить операцию записи по межстрановой ссылке, но страницы, читающие из этих кластеров баз данных, возвращали актуальные результаты, если запрос на чтение попадал на только что восстановленную реплику. Другие более крупные кластеры баз данных все еще восстанавливались.

Наши команды определили способы восстановления непосредственно с западного побережья, чтобы преодолеть ограничения пропускной способности, вызванные загрузкой из внешнего хранилища, и все больше убеждались в том, что восстановление неизбежно, а время, оставшееся до установления работоспособной топологии репликации, зависело от того, как долго она будет работать. взять репликацию, чтобы наверстать упущенное. Эта оценка была линейно интерполирована на основе имеющейся у нас телеметрии репликации, а страница состояния была обновлена , чтобы установить ожидаемое время восстановления в два часа.

#### 22 октября 2018 07:46 UTC
GitHub опубликовал сообщение в блоге , чтобы предоставить больше контекста. Мы используем GitHub Pages для внутренних целей, и все сборки были приостановлены несколькими часами ранее, поэтому публикация этого потребовала дополнительных усилий. Мы приносим извинения за задержку. Мы намеревались разослать это сообщение намного раньше и позаботимся о том, чтобы в будущем мы могли публиковать обновления с учетом этих ограничений.

#### 22 октября 2018 г., 11:12 UTC
Все первичные базы данных снова установлены на восточном побережье США. Это привело к тому, что сайт стал гораздо более отзывчивым, так как записи теперь направлялись на сервер базы данных, расположенный в том же физическом центре обработки данных, что и наш уровень приложений. Несмотря на то, что это существенно повысило производительность, по-прежнему существовали десятки реплик чтения базы данных, которые отставали от основной на несколько часов. Эти отложенные реплики приводили к тому, что пользователи видели несогласованные данные при взаимодействии с нашими службами. Мы распределили нагрузку чтения по большому пулу реплик чтения, и каждый запрос к нашим службам имел хорошие шансы попасть в реплику чтения, которая задерживалась на несколько часов.

На самом деле время, необходимое для репликации, соответствовало функции убывания мощности, а не линейной траектории. Из-за увеличения нагрузки записи на наши кластеры баз данных, когда пользователи просыпались и начинали свой рабочий день в Европе и США, процесс восстановления занял больше времени, чем первоначально предполагалось.

#### 22 октября 2018 г., 13:15 UTC
К настоящему времени мы приближались к пиковой нагрузке трафика на GitHub.com. Группа реагирования на инциденты обсудила, как действовать дальше. Было ясно, что задержки репликации увеличиваются, а не уменьшаются в направлении согласованного состояния. Мы начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке восточного побережья США ранее во время инцидента. Как только они стали доступны, стало проще распределять объем запросов на чтение по большему количеству серверов. Сокращение совокупного использования реплик чтения позволило репликации наверстать упущенное.

#### 22 октября 2018 г., 16:24 UTC
Как только реплики были синхронизированы, мы выполнили аварийное переключение на исходную топологию, решив немедленные проблемы с задержкой/доступностью. В рамках сознательного решения отдавать предпочтение целостности данных, а не более короткому периоду инцидента, мы сохранили статус службы красным , пока начали обрабатывать накопившиеся данные.

#### 22 октября 2018 г., 16:45 (всемирное координированное время)
На этом этапе восстановления нам нужно было сбалансировать возросшую нагрузку, представленную отставанием, потенциально перегружая наших партнеров по экосистеме уведомлениями, и как можно быстрее вернуть наши услуги на 100%. В очереди было более пяти миллионов событий ловушек и 80 тысяч сборок страниц.

Когда мы повторно включили обработку этих данных, мы обработали около 200 000 полезных нагрузок веб-перехватчиков, которые пережили внутренний TTL и были удалены. Обнаружив это, мы приостановили эту обработку и внесли изменение, чтобы на время увеличить этот TTL.

Чтобы избежать дальнейшего подрыва надежности наших обновлений статуса, мы оставались в ухудшенном статусе до тех пор, пока не завершили обработку всего отставания данных и не убедились, что наши сервисы явно вернулись к нормальному уровню производительности.

#### 22 октября 2018 23:03 UTC
Все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа всех систем. Статус сайта был обновлен до зеленого .

### Следующие шаги
#### Устранение несоответствий данных
Во время нашего восстановления мы захватили двоичные журналы MySQL, содержащие записи, которые мы сделали на нашем основном сайте, но которые не были реплицированы на наш сайт Западного побережья из каждого затронутого кластера. Общее количество операций записи, которые не были воспроизведены на Западном побережье, было относительно небольшим. Например, один из наших самых загруженных кластеров имел 954 записи в затронутом окне. В настоящее время мы проводим анализ этих журналов и определяем, какие записи могут быть автоматически согласованы, а какие потребуют взаимодействия с пользователями. У нас есть несколько команд, занятых этой работой, и наш анализ уже определил категорию записей, которые с тех пор повторялись пользователем и успешно сохранялись. Как указано в этом анализе, наша основная цель — сохранить целостность и точность данных, которые вы храните на GitHub.

### Коммуникация
Желая сообщить вам важную информацию во время инцидента, мы сделали несколько публичных оценок времени ремонта на основе скорости обработки невыполненных данных. Оглядываясь назад, наши оценки не учитывали все переменные. Мы приносим извинения за возникшую путаницу и постараемся предоставить более точную информацию в будущем.

### Технические инициативы
В ходе этого анализа был выявлен ряд технических инициатив. По мере того, как мы продолжаем проводить обширный внутренний анализ после инцидента, мы рассчитываем определить еще больше работы, которую необходимо выполнить.

Настройте конфигурацию Orchestrator, чтобы предотвратить продвижение основных баз данных через региональные границы. Действия Оркестратора вели себя так, как настроено, несмотря на то, что наш уровень приложений не смог поддержать это изменение топологии. Выборы лидеров в регионе, как правило, безопасны, но внезапная задержка между странами стала основным фактором, способствовавшим этому инциденту. Это было неожиданное поведение системы, учитывая, что ранее мы не видели внутреннего сетевого раздела такого масштаба.
Мы ускорили переход на новый механизм отчетов о состоянии, который предоставит нам более богатый форум для обсуждения активных инцидентов более четким и понятным языком. Хотя во время инцидента многие части GitHub были доступны, мы смогли установить только зеленый, желтый и красный статус. Мы понимаем, что это не дает вам точного представления о том, что работает, а что нет, и в будущем мы будем отображать различные компоненты платформы, чтобы вы знали статус каждой службы.
За несколько недель до этого инцидента мы начали общекорпоративную инженерную инициативу по поддержке обслуживания трафика GitHub из нескольких центров обработки данных в схеме «активный/активный/активный». Целью этого проекта является поддержка резервирования N+1 на уровне объекта. Цель этой работы — допустить полный отказ одного центра обработки данных без воздействия на пользователя. Это серьезное усилие, которое займет некоторое время, но мы считаем, что наличие нескольких сайтов с хорошей связью в одном регионе обеспечивает хороший набор компромиссов. Этот инцидент добавил актуальности инициативе.
Мы займем более активную позицию в проверке наших предположений. GitHub — быстрорастущая компания, и за последнее десятилетие она значительно усложнилась. По мере того, как мы продолжаем расти, становится все труднее фиксировать и передавать исторический контекст компромиссов и решений, принятых новым поколениям Хабберов.
Организационные инициативы
Этот инцидент изменил наше отношение к надежности сайта. Мы узнали, что более строгий операционный контроль или сокращение времени отклика не являются достаточными гарантиями надежности сайта в рамках такой сложной системы услуг, как наша. Чтобы поддержать эти усилия, мы также начнем системную практику проверки сценариев сбоев, прежде чем они смогут повлиять на вас. Эта работа потребует будущих инвестиций в инструменты внедрения ошибок и хаос-инжиниринга в GitHub.

### Вывод
Мы знаем, насколько вы полагаетесь на GitHub для успеха своих проектов и бизнеса. Никто так не заинтересован в доступности наших услуг и правильности ваших данных. Мы продолжим анализировать это событие, чтобы найти возможности для того, чтобы лучше обслуживать вас и заслужить доверие, которое вы нам оказываете.

**Ответ:**

1. Краткое описание инцидента
2. Предшествующие события
* Предпосылки - критически важное упущение
Топология центра обработки данных предназначена для обеспечения надежной и расширяемой пограничной сети, которая работает перед несколькими региональными центрами обработки данных, обеспечивающими наши рабочие нагрузки вычислений и хранения. Несмотря на уровни избыточности, встроенные в физические и логические компоненты в этой конструкции, все еще возможно, что сайты не смогут взаимодействовать друг с другом в течение некоторого времени.

3. Причина инцидента

21 октября в 22:52 UTC 
в результате регламентных работ по замене вышедшего из строя оптического оборудования 100G была потеряна связь между нашим сетевым концентратором на восточном побережье США и нашим основным центром обработки данных на восточном побережье США. Связь между этими точками была восстановлена за 43 секунды, но этот кратковременный сбой вызвал цепочку событий, которые привели к ухудшению качества обслуживания на 24 часа 11 минут.

4. Воздействие

2018 21 октября 22:52 UTC
* Воздействие
Согласно консенсусу Raft, активный Orchestrator начал процесс отмены выбора руководства. Западный ЦОД и узлы Orchestrator общедоступного облака Восточного ЦОД смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США. 

Orchestrator приступил к организации топологии кластера базы данных Западного побережья США. Когда подключение было восстановлено, наш уровень приложений немедленно начал направлять трафик записи на новые первичные серверы на сайте West Coast.

* Воздействие
Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который не был реплицирован на объект на западном побережье США. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить возврат основного сервера в центр обработки данных на восточном побережье США.

5. Обнаружение
6. Реакция
7. Восстановление
8. Таймлайн 
9. Последующие действия


1. Краткое описание инцидента (краткая выжимка о инциденте)

Из-за кратковременного отсутствия связи между Восточным и Западным ЦОД кластеры БД стали содержать различные данные за период отсутствия связи (43 секунды). После возобновления связи кластеры БД для восстановления целостности БД начали процесс репликации с Восточного ЦОД на Западный и с Западного ЦОД на Восточный. Это привело к высокой нагрузке на каналы связи, повышенной задержке при передаче данных (латентности) и непредсказуемому времени восстановления целостности БД из-за режима перерепликации. Для преодоления данной ситуации была выполнена временная приостановка функции записи новых данных в БД, что снизило трафик репликации и снизило латентность. В ручном режиме были восстановлены данные в Восточном ЦОД. Затем включен режим репликации с Западным ЦОД. Когда доступность сайта восстановилась до нормы, включен режим записи новых данных и восстановлены все сервисы сайта.

2. Предшествующие события (что произошло перед инцидентом)

Вышло из строя оптическое оборудование 100G. При проектировании топологии кластера отсутсвие условий для резервирования сетевого оборудования в ЦОД.

3. Причина инцидента (из-за чего возник инцидент)

Пропала связь с сетевым концентраторм Восточного ЦОД. Отсутствие оборудования для резервирования затронутой аварией линиии связи.

4. Воздействие (на что повлиял инцидент)

Произошло разделение записей в БД сайта. Восточный ЦОД и Западный ЦОД стали содержать различные данные.

5. Обнаружение (когда и как инцидент был обнаружен)

Внутренние системы мониторинга генерировали предупреждения, указывающие на многочисленные сбои в системах. Иженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. 

6. Реакция (кто ответил на инцидент, кто был привлечен, какие каналы коммуникации были задействованы)

Инженеры, участвующие в группе реагирования на инциденты, начали разработку плана по устранению несоответствий данных и внедрению наших процедур аварийного переключения для MySQL.
К решению проблемы присоединился координатор инцидента.
Были вызваны дополнительные инженеры из группы разработки баз данных GitHub. 
В блоге были сообщения для пользователей о статусе сайта и планируемом времени восстановления полной работоспособности всех сервисов сайта.

* Реакция
В то время было понятно, что проблема затронула несколько кластеров баз данных. 

Были вызваны дополнительные инженеры из группы разработки баз данных GitHub. 

Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. 

Это было непросто, потому что к этому моменту кластер базы данных West Coast уже почти 40 минут принимал записи с нашего уровня приложений. 

Кроме того, в кластере Восточного побережья существовало несколько секунд операций записи, которые не были реплицированы на Западное побережье, что предотвратило репликацию новых операций записи обратно на Восточное побережье.

* Реакция
Инженеры, участвующие в группе реагирования на инциденты, **начали разработку плана по устранению несоответствий данных** и внедрению наших процедур аварийного переключения для MySQL. Наш план состоял в том, чтобы **восстановить из резервных копий**, **синхронизировать реплики на обоих сайтах**, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди. 
Мы **обновили наш статус** , чтобы проинформировать пользователей о том, что мы собираемся выполнить контролируемый переход на другой ресурс внутренней системы хранения данных.


7. Восстановление (описание действий по устранению инцидента и поведение системы)



#### 8. Таймлайн (последовательное описание ключевых событий инцидента с указанием времени)

* 2018 21 октября 22:52 UTC
> Из-за кратковременного отсутствия связи между Восточным и Западным ЦОД кластеры БД начали процесс репликации для восстановления целостности БД.
> Резко увеличился трафик между ЦОД и повысилась дополнительная задержка в передаче данных между ЦОД.


* Воздействие

Согласно консенсусу Raft, во время описанного выше раздела сети Orchestrator, который был активен в нашем основном центре обработки данных, начал процесс отмены выбора руководства. 

Центр обработки данных на западном побережье США и узлы Orchestrator общедоступного облака на восточном побережье США смогли установить кворум и начать аварийное переключение кластеров для направления операций записи в центр обработки данных на западном побережье США. 

Orchestrator приступил к организации топологии кластера базы данных Западного побережья США. 

Когда подключение было восстановлено, наш уровень приложений немедленно начал направлять трафик записи на новые первичные серверы на сайте West Coast.


Серверы баз данных в центре обработки данных на восточном побережье США содержали короткий период записи, который не был реплицирован на объект на западном побережье США. 

Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали записи, которых не было в другом центре обработки данных, мы не смогли безопасно выполнить возврат основного сервера в центр обработки данных на восточном побережье США.




* 2018 21 октября 22:54 UTC
> системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах


* 2018 21 октября 23:02 UTC
> Инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии.
> Несколько инженеров отвечали и работали над сортировкой входящих уведомлений.



* Обнаружение
Наши внутренние системы мониторинга начали генерировать предупреждения, указывающие на многочисленные сбои в наших системах. 

В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. 

К 23:02 UTC инженеры нашей группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии. 

Запрос API Orchestrator показал топологию репликации базы данных, которая включала только серверы из нашего центра обработки данных на западном побережье США.


* 2018 21 октября 23:07 UTC

> Отвечающая команда решила вручную заблокировать наш внутренний инструмент развертывания, чтобы предотвратить внесение каких-либо дополнительных изменений. 

* 2018 21 октября 23:09 UTC 
> Команда респондентов поместила сайт в желтый статус . Это действие автоматически переводит ситуацию в активный инцидент и отправляет предупреждение координатору инцидентов. 

* 2018 21 октября 23:11 UTC 
> Присоединился координатор инцидента и через две минуты изменил статус решения на красный .

* 2018 21 октября 23:13 UTC
> Инженеры из группы разработки баз данных GitHub приступили к ручной настройке БД Восточного кластера в качестве основной для каждого кластера и перестройки топологии репликации. 
> А также они приостановили репликацию тех записей, которые не были реплицированы на Западное побережье, что предотвратило репликацию новых операций записи обратно на Восточное побережье. 
> Основание такого решения - невозможность справиться с **дополнительной задержкой**, возникающей из-за обмена данными между странами для большинства их вызовов базы данных. Это решение приведет к тому, что сервис станет непригодным для многих пользователей.


* Реакция
В то время было понятно, что проблема затронула несколько кластеров баз данных. 

Были вызваны дополнительные инженеры из группы разработки баз данных GitHub. 

Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. 

Это было непросто, потому что к этому моменту кластер базы данных West Coast уже почти 40 минут принимал записи с нашего уровня приложений. 

Кроме того, в кластере Восточного побережья существовало несколько секунд операций записи, которые не были реплицированы на Западное побережье, что предотвратило репликацию новых операций записи обратно на Восточное побережье.

* Пояснение причин
Защита конфиденциальности и целостности пользовательских данных является наивысшим приоритетом GitHub. Стремясь сохранить эти данные, мы решили, что более 30 минут данных, записанных в центр обработки данных на западном побережье США, не позволили нам рассмотреть другие варианты, кроме переадресации при сбое, для обеспечения безопасности пользовательских данных. 

* Пояснение причин
Однако приложения, работающие на восточном побережье и зависящие от записи информации в кластер MySQL на западном побережье, в настоящее время не в состоянии справиться с **дополнительной задержкой**, возникающей из-за обмена данными между странами для большинства их вызовов базы данных. Это решение приведет к тому, что наш сервис станет непригодным для многих пользователей. Мы считаем, что длительная деградация сервиса стоила обеспечения согласованности данных наших пользователей.


#### 2018 21 октября 23:19 UTC
> Приостановлены сервисы доставки веб-перехватчика и сборки GitHub Pages для сохранения целостности данных.
> 


* Реакция
Из запроса состояния кластеров базы данных стало ясно, что нам нужно остановить выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления. Мы сделали явный выбор частично ухудшить удобство использования сайта, **приостановив доставку веб-перехватчика и сборки GitHub Pages** вместо того, чтобы подвергать опасности данные, которые мы уже получили от пользователей. Другими словами, наша стратегия заключалась в том, чтобы отдавать предпочтение целостности данных, а не удобству использования сайта и времени восстановления.

#### 22 октября 2018 г., 00:05 UTC
> Начало разработки плана по устранению несоответствий данных и синхронизации реплик на обоих сайтах.
> Обновлен статус сайта.
> Проинформировали пользователей о том, что планируется выполнить контролируемый переход на другой ресурс внутренней системы хранения данных.



* Реакция
Инженеры, участвующие в группе реагирования на инциденты, **начали разработку плана по устранению несоответствий данных** и внедрению наших процедур аварийного переключения для MySQL. Наш план состоял в том, чтобы **восстановить из резервных копий**, **синхронизировать реплики на обоих сайтах**, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди. 
Мы **обновили наш статус** , чтобы проинформировать пользователей о том, что мы собираемся выполнить контролируемый переход на другой ресурс внутренней системы хранения данных.

* Пояснение
Хотя резервные копии данных MySQL создаются каждые четыре часа и хранятся в течение многих лет, резервные копии хранятся удаленно в общедоступном облачном хранилище BLOB-объектов. 

**Время, необходимое для восстановления нескольких терабайт данных резервной копии, привело к тому, что процесс занял несколько часов**. 

Значительная часть времени ушла на перенос данных из службы удаленного резервного копирования. 

**Процесс распаковки, контрольной суммы, подготовки и загрузки больших файлов резервных копий на недавно подготовленные серверы MySQL занял большую часть времени**. 

Эта процедура тестируется как минимум ежедневно, поэтому сроки восстановления были хорошо известны, однако до этого инцидента нам никогда не приходилось полностью восстанавливать весь кластер из резервной копии, и вместо этого мы могли полагаться на другие стратегии, такие как отложенные реплики.

#### 22 октября 2018 г., 00:41 UTC
> Начало процесса резервного копирования для всех затронутых кластеров MySQL.
> Поиск решения для ускорения резервного копирования.

* Реакция
К этому времени был инициирован процесс резервного копирования для всех затронутых кластеров MySQL, и инженеры следили за его ходом. 

Одновременно несколько групп инженеров искали способы ускорить передачу и время восстановления без дальнейшего ухудшения удобства использования сайта или риска повреждения данных.

#### 22 октября 2018 г., 06:51 UTC
> Завершение восстановление некоторых кластеров из резервных копий в Восточном ЦОД и начало репликации с Западным ЦОД.
> Изменение статуса сайта "ожидаемое время восстановления - два часа"


* Восстановление
Несколько кластеров завершили восстановление из резервных копий в нашем центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья. 

Это привело к медленной загрузке сайта для страниц, которые должны были выполнить операцию записи по межстрановой ссылке, но страницы, читающие из этих кластеров баз данных, возвращали актуальные результаты, если запрос на чтение попадал на только что восстановленную реплику. 

Другие более крупные кластеры баз данных все еще восстанавливались.

Наши команды определили способы восстановления непосредственно с западного побережья, чтобы преодолеть ограничения пропускной способности, вызванные загрузкой из внешнего хранилища, и все больше убеждались в том, что восстановление неизбежно, а время, оставшееся до установления работоспособной топологии репликации, зависело от того, как долго она будет работать. 

Взять репликацию, чтобы наверстать упущенное. 

Эта оценка была линейно интерполирована на основе имеющейся у нас телеметрии репликации, а страница состояния была обновлена , чтобы установить ожидаемое время восстановления в два часа.

#### 22 октября 2018 07:46 UTC
> Опубликование сообщения в блоге для подачи актального окнтента пользователям.


GitHub опубликовал сообщение в блоге , чтобы предоставить больше контекста. 

Мы используем GitHub Pages для внутренних целей, и все сборки были приостановлены несколькими часами ранее, поэтому публикация этого потребовала дополнительных усилий. 

Мы приносим извинения за задержку. 

Мы намеревались разослать это сообщение намного раньше и позаботимся о том, чтобы в будущем мы могли публиковать обновления с учетом этих ограничений.

#### 22 октября 2018 г., 11:12 UTC
> Установление всех первичных БД на Восточном ЦОД
> Повышение отзывчивости сайта
> Распределение нагрузки чтения по большому пулу реплик чтения

Все первичные базы данных снова установлены на восточном побережье США. 

Это привело к тому, что сайт стал гораздо более отзывчивым, так как записи теперь направлялись на сервер базы данных, расположенный в том же физическом центре обработки данных, что и наш уровень приложений. 

Несмотря на то, что это существенно повысило производительность, по-прежнему существовали десятки реплик чтения базы данных, которые отставали от основной на несколько часов. 

Эти отложенные реплики приводили к тому, что пользователи видели несогласованные данные при взаимодействии с нашими службами. 

Мы распределили нагрузку чтения по большому пулу реплик чтения, и каждый запрос к нашим службам имел хорошие шансы попасть в реплику чтения, которая задерживалась на несколько часов.

На самом деле время, необходимое для репликации, соответствовало функции убывания мощности, а не линейной траектории. 

Из-за увеличения нагрузки записи на наши кластеры баз данных, когда пользователи просыпались и начинали свой рабочий день в Европе и США, процесс восстановления занял больше времени, чем первоначально предполагалось.

#### 22 октября 2018 г., 13:15 UTC
> Начало предоставления дополнительных реплик чтения MySQL в общедоступном облаке Восточного ЦОД для сокращения совокупного использования реплик
> 


К настоящему времени мы приближались к пиковой нагрузке трафика на GitHub.com. 

Группа реагирования на инциденты обсудила, как действовать дальше. 

Было ясно, что задержки репликации увеличиваются, а не уменьшаются в направлении согласованного состояния. 

Мы начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке восточного побережья США ранее во время инцидента. 

Как только они стали доступны, стало проще распределять объем запросов на чтение по большему количеству серверов. 

Сокращение совокупного использования реплик чтения позволило репликации наверстать упущенное.



#### 22 октября 2018 г., 16:24 UTC
> Выполнено аварийное переключение на исходную топологию.

Как только реплики были синхронизированы, мы выполнили аварийное переключение на исходную топологию, решив немедленные проблемы с задержкой/доступностью. 

В рамках сознательного решения отдавать предпочтение целостности данных, а не более короткому периоду инцидента, мы сохранили статус службы красным , пока начали обрабатывать накопившиеся данные.

#### 22 октября 2018 г., 16:45
> Корректировка TTL веб-перехватчиков для предотвращения удаления полезных нагрузок веб-перехватчиков.
> Сохранение красного статуса сайта.


На этом этапе восстановления нам нужно было сбалансировать возросшую нагрузку, представленную отставанием, потенциально перегружая наших партнеров по экосистеме уведомлениями, и как можно быстрее вернуть наши услуги на 100%. В очереди было более пяти миллионов событий ловушек и 80 тысяч сборок страниц.

Когда мы повторно включили обработку этих данных, мы обработали около 200 000 полезных нагрузок веб-перехватчиков, которые пережили внутренний TTL и были удалены. 

Обнаружив это, мы приостановили эту обработку и внесли изменение, чтобы на время увеличить этот TTL.

Чтобы избежать дальнейшего подрыва надежности наших обновлений статуса, мы оставались в ухудшенном статусе до тех пор, пока не завершили обработку всего отставания данных и не убедились, что наши сервисы явно вернулись к нормальному уровню производительности.

#### 22 октября 2018 23:03 UTC
> Все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа всех систем. 
> Статус сайта был обновлен до зеленого.

9. Последующие действия (что нужно предпринять, чтобы инцидент не повторялся)




## Задача повышенной сложности - Задача отменена

Прослушайте [симуляцию аудиозаписи о инциденте](https://youtu.be/vw6I5DYWkNA?t=1), предоставленную 
разработчиками инструмента для автоматизации инцидент-менеджмента PagerDuty.

На основании этой аудиозаписи попробуйте составить сообщения для пользователей о данном инциденте.

Должно быть 3 сообщения о:
- начале инцидента
- продолжающихся работах
- окончании инцидента и возвращении к штатной работе

---

### Как оформить ДЗ?

Выполненное домашнее задание пришлите ссылкой на .md-файл в вашем репозитории.

---
